{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "----\n",
    "\n",
    "<center><h3>Suriyadeepan Ramamoorthy</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "---\n",
    "\n",
    "- Text Represenation\n",
    "- Reduction / Abstraction\n",
    "- Articulation / Synthesis\n",
    "- Reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Overview\n",
    "---\n",
    "\n",
    "- Text Represenation\n",
    "- Reduction / Abstraction\n",
    "- ~~Articulation / Synthesis~~\n",
    "- ~~Reasoning~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Represenation\n",
    "---\n",
    "- Count-based Representation\n",
    "- Continuous Represetation\n",
    "    - Example : Continuous Bag-of-Words\n",
    "- Text Preprocessing\n",
    "    - Example : Social Media Sentiment Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reduction / Abstraction\n",
    "---\n",
    "- Neural Networks\n",
    "- Example : Neural Networks from Scratch in pytorch\n",
    "- Recurrent Neural Networks\n",
    "    - LSTM (Long Short Term Memory)\n",
    "- Example : Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Count-based Representation\n",
    "---\n",
    "\n",
    "- One-hot Encoding\n",
    "- Bag-of-Words\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One-hot Encoding\n",
    "\n",
    "```python\n",
    "vocab = [ 'one', 'two', 'three' , 'four' ]\n",
    "```\n",
    "```\n",
    "one   : tensor([1., 0., 0., 0.])\n",
    "two   : tensor([0., 1., 0., 0.])\n",
    "three : tensor([0., 0., 1., 0.])\n",
    "four  : tensor([0., 0., 0., 1.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def one_hot_encode(w, w2i):\n",
    "  x = torch.zeros(len(w2i))\n",
    "  idx = w2i[w]\n",
    "  x[idx] = 1\n",
    "  return x\n",
    "\n",
    "vocab = [ 'one', 'two', 'three' , 'four' ]\n",
    "w2i = { w: i for i, w in enumerate(vocab) }\n",
    "for w in vocab:\n",
    "    print(w, ':', one_hot_encode(w, w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# N-gram\n",
    "\n",
    "```python\n",
    "sentence = 'one two three four'\n",
    "```\n",
    "\n",
    "```\n",
    "['one',\n",
    " 'two',\n",
    " 'three',\n",
    " 'four',\n",
    " 'one two',\n",
    " 'two three',\n",
    " 'three four',\n",
    " 'one two three',\n",
    " 'two three four']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('one two three four')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Count Vectorization\n",
    "\n",
    "\n",
    "```\n",
    "Size of vocabulary :  122\n",
    "Sentence :  The last question was asked for the first time, half in jest, on May 21, 2061, at a time when humanity first stepped into the light.\n",
    "Vector :  [[1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    "  0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0\n",
    "  0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 3 0 0 0\n",
    "  2 0 0 0 0 1 0 0 0 0 1 0 0 0]]\n",
    "Most Frequent Word :  (104, 'the')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# create an instance of sklearn's Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "sentences = sent_tokenize(open('data/asimov-tiny.txt').read())\n",
    "vectorizer.fit(sentences)\n",
    "w2i = vectorizer.vocabulary_\n",
    "vocab = { i: w for w, i in w2i.items() }\n",
    "print('Size of vocabulary : ', len(vocab))\n",
    "print('Sentence : ', sentences[0])\n",
    "sent_vector = vectorizer.transform([sentences[0]]).toarray()\n",
    "print('Vector : ', sent_vector)\n",
    "most_frequent = np.argmax(sent_vector[-1])\n",
    "print('Most Frequent Word : ', (w2i['the'], vocab[104]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TF-IDF\n",
    "---\n",
    "\n",
    "$$tf \\times idf$$\n",
    "- $tf$ : Term Frequency in the document\n",
    "- $idf$ : (logarithm of) inverse fraction of the documents that contain the word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# create an instance of sklearn's Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "sentences = sent_tokenize(open('data/asimov-tiny.txt').read())\n",
    "vectorizer.fit(sentences)\n",
    "vectors = vectorizer.transform(sentences)\n",
    "tf_transformer = TfidfTransformer()\n",
    "tf_transformer.fit(vectors)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "w2i = vectorizer.vocabulary_\n",
    "vocab = { i: w for w, i in w2i.items() }\n",
    "print('Size of vocabulary : ', len(vocab))\n",
    "print('Sentence : ', sentences[0])\n",
    "sent_vector = vectorizer.transform([sentences[0]]).toarray()\n",
    "print('Vector : ', sent_vector)\n",
    "most_frequent = np.argmax(sent_vector[-1])\n",
    "print(tf_transformer.transform(sent_vector).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "\n",
    "```\n",
    "[[0.19564323 0.19564323 0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.19564323 0.1639643  0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.39128647\n",
    "  0.         0.         0.1639643  0.         0.         0.\n",
    "  0.         0.         0.         0.         0.19564323 0.\n",
    "  0.         0.         0.         0.19564323 0.1639643  0.19564323\n",
    "  0.         0.         0.         0.19564323 0.         0.19564323\n",
    "  0.         0.         0.19564323 0.         0.         0.\n",
    "  0.         0.19564323 0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.19564323 0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.1639643  0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.19564323\n",
    "  0.         0.         0.26199672 0.         0.         0.\n",
    "  0.39128647 0.         0.         0.         0.         0.14148774\n",
    "  0.         0.         0.         0.         0.19564323 0.\n",
    "  0.         0.        ]]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch for Machine Learning\n",
    "---\n",
    "\n",
    "- Logistic Regression\n",
    "- Neural Network\n",
    "- Recurrent Neural Network\n",
    "- Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "---\n",
    "\n",
    "| x | y   |\n",
    "|------|------|\n",
    "| 0.54  | 2.01    |\n",
    "| 1.21  | 4.13    |\n",
    "| 0.2   | 0.82    |\n",
    "| ...   | ...     |\n",
    "\n",
    "$$\\hat{y} = wx + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.rand(1)\n",
    "linear = nn.Linear(1, 1)\n",
    "y = linear(x)\n",
    "print('x : ', x)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ', list(linear.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "```\n",
    "x :  tensor([0.3726])\n",
    "y :  tensor([-0.6245], grad_fn=<AddBackward0>)\n",
    "Associated Parameters :  [('weight', Parameter containing:\n",
    "tensor([[0.1961]], requires_grad=True)), ('bias', Parameter containing:\n",
    "tensor([-0.6975], requires_grad=True))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-variate Linear Regression\n",
    "---\n",
    "\n",
    "| $x_0$ | $x_1$   | $x_2$ | y   |\n",
    "|------|------|------|------|\n",
    "| 0.54  | 2.01    | 0.14  | 2.91    |\n",
    "| 1.21  | 4.13    | 0.24  | 4.22    |\n",
    "| 0.2   | 0.82    | 1.8   | 1.35    |\n",
    "| ...   | ...     | ...   | ...     |\n",
    "\n",
    "$$\\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b$$\n",
    "<center><h3>OR</h3></center>\n",
    "$$\\hat{\\overrightarrow{y}} = W\\overrightarrow{x} + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.rand(1, 3)\n",
    "linear = nn.Linear(3, 1)\n",
    "y = linear(x)\n",
    "print('x : ', x)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ', list(linear.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-variate Linear Regression\n",
    "\n",
    "```\n",
    "x :  tensor([[0.2249, 0.2030, 0.8778]])\n",
    "y :  tensor([[0.0854]], grad_fn=<AddmmBackward>)\n",
    "Associated Parameters :  [('weight', Parameter containing:\n",
    "tensor([[ 0.4610, -0.3530,  0.5720]], requires_grad=True)), ('bias', Parameter containing:\n",
    "tensor([-0.4488], requires_grad=True))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "---\n",
    "$$\\hat{y} = \\sigma( wx + b )$$\n",
    "$$ y \\epsilon (0, 1)\\ \\forall\\ x \\epsilon (-\\infty, +\\infty) $$\n",
    "\n",
    "![](doc/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "x = torch.rand(1)\n",
    "linear = nn.Linear(1, 1)\n",
    "activation_fn = nn.Sigmoid()\n",
    "# activation_fn = nn.Tanh()\n",
    "y = activation_fn(linear(x))\n",
    "print('x : ', x)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ', list(linear.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "```\n",
    "x :  tensor([0.4730])\n",
    "y :  tensor([0.6162], grad_fn=<SigmoidBackward>)\n",
    "Associated Parameters :  [('weight', Parameter containing:\n",
    "tensor([[0.1867]], requires_grad=True)), ('bias', Parameter containing:\n",
    "tensor([0.3851], requires_grad=True))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Network\n",
    "---\n",
    "\n",
    "- Feed-Forward Neural Network\n",
    "- Add a hidden layer\n",
    "- 1 input layer, 1 hidden layer, 1 output layer\n",
    "\n",
    "$$ \\hat{y} = \\tanh(W_2z + b_2) $$\n",
    "$$ z = \\tanh(W_1x + b_1) $$\n",
    "\n",
    "![](doc/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "x = torch.rand(1, 3)\n",
    "linear_1 = nn.Linear(3, 5)\n",
    "activation_fn_1 = nn.Tanh()\n",
    "linear_2 = nn.Linear(5, 1)\n",
    "activation_fn_2 = nn.Tanh()\n",
    "z = activation_fn_1(linear_1(x))\n",
    "y = activation_fn_2(linear_2(z))\n",
    "print('x : ', x)\n",
    "print('z : ', z)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ')\n",
    "print('\\nlinear_1 (weight) : ', \n",
    "      linear_1.weight.size(), linear_1.weight)\n",
    "print('linear_1 (bias) : ', \n",
    "      linear_1.bias.size(), linear_1.bias)\n",
    "print('\\nlinear_2 (weight) : ', \n",
    "      linear_2.weight.size(), linear_2.weight)\n",
    "print('linear_2 (bias) : ', \n",
    "      linear_2.bias.size(), linear_2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "```\n",
    "linear_1 (weight) :  torch.Size([5, 3]) Parameter containing:\n",
    "tensor([[ 0.0067, -0.0129, -0.3105],\n",
    "        [ 0.0487, -0.5520, -0.5208],\n",
    "        [-0.5579, -0.0674, -0.3417],\n",
    "        [-0.2760, -0.2374,  0.4286],\n",
    "        [ 0.1238,  0.5067, -0.0352]], requires_grad=True)\n",
    "linear_1 (bias) :  torch.Size([5]) Parameter containing:\n",
    "tensor([-0.4454,  0.5320,  0.0727,  0.1670, -0.0999], requires_grad=True)\n",
    "\n",
    "linear_2 (weight) :  torch.Size([1, 5]) Parameter containing:\n",
    "tensor([[ 0.1594,  0.2691, -0.1408,  0.1989,  0.4021]], requires_grad=True)\n",
    "linear_2 (bias) :  torch.Size([1]) Parameter containing:\n",
    "tensor([0.2987], requires_grad=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Network\n",
    "---\n",
    "\n",
    "$$h_t = W_{hh}h_{t-1} + W_{xh}x_{t}$$\n",
    "$$y_t = softmax(W_{hy}h_t)$$\n",
    "\n",
    "![](doc/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Network\n",
    "---\n",
    "\n",
    "![](doc/simple_seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.2284, 0.7716]], grad_fn=<SoftmaxBackward>), tensor([[0.2077, 0.7923]], grad_fn=<SoftmaxBackward>), tensor([[0.2070, 0.7930]], grad_fn=<SoftmaxBackward>), tensor([[0.2067, 0.7933]], grad_fn=<SoftmaxBackward>), tensor([[0.1817, 0.8183]], grad_fn=<SoftmaxBackward>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "input = torch.rand([1, 5, 3])\n",
    "output_size = 2\n",
    "state = torch.zeros(1, 3)\n",
    "ss, si = nn.Linear(3, 3), nn.Linear(3, 3)\n",
    "so = nn.Linear(3, 3)\n",
    "linear_final = nn.Linear(3, output_size)\n",
    "y = []\n",
    "for i in range(5):\n",
    "    state = ss(state) + si(input.permute(1, 0, 2)[i])\n",
    "    out = so(state)\n",
    "    y.append(F.softmax(linear_final(out)))\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "---\n",
    "\n",
    "![](doc/lstm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "---\n",
    "\n",
    "![](doc/lstm_eqn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM\n",
    "\n",
    "```python\n",
    "class LstmClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, hparams, weights=None):\n",
    "    \"\"\" (hparams : dictionary of hyperparameters) \"\"\"\n",
    "    super(LstmClassifier, self).__init__()\n",
    "    self.hparams = hparams\n",
    "    self.weights = weights\n",
    "    self.embedding = nn.Embedding(hparams['vocab_size'], hparams['emb_dim'])\n",
    "    self.lstm = nn.LSTM(hparams['emb_dim'], hparams['hidden_dim'])\n",
    "    self.linear = nn.Linear(hparams['hidden_dim'], hparams['output_size'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM\n",
    "\n",
    "```python\n",
    "  def forward(self, sequence, batch_size=None, get_hidden=False):\n",
    "    input = self.embedding(sequence)\n",
    "    input = input.permute(1, 0, 2)\n",
    "    batch_size = batch_size if batch_size else self.hparams['batch_size']\n",
    "    h0 = torch.zeros(1, batch_size, self.hparams['hidden_dim'])\n",
    "    c0 = torch.zeros(1, batch_size, self.hparams['hidden_dim'])\n",
    "    self.lstm.flatten_parameters()\n",
    "    lstm_out, (h, c) = self.lstm(input, (h0, c0))\n",
    "    self.h = h[-1]\n",
    "    linear_out = self.linear(h[-1])\n",
    "    return linear_out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing\n",
    "---\n",
    "\n",
    "- Tokenization\n",
    "- Indexing\n",
    "- Padding\n",
    "- Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'I', \"'m\", 'Suriya']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"Hi, I'm Suriya\"\n",
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 3, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_index(w, w2i, UNK_IDX=1):\n",
    "    return w2i[w] if w in w2i else UNK_IDX\n",
    "\n",
    "def index(sentence, w2i):\n",
    "    assert isinstance(sentence, type([]))\n",
    "    return [ word_to_index(token, w2i) for token in sentence ]\n",
    "\n",
    "w2i = {'one' : 2, 'two' : 3, 'three' : 4, 'four' : 5}\n",
    "tokenized_sentence = ['zero', 'three', 'two', 'five']\n",
    "index(tokenized_sentence, w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 0],\n",
       "         [2, 3, 0],\n",
       "         [4, 5, 6]]), tensor([1, 1, 0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad(samples, max_len=None, PAD_IDX=0, to_tensor=torch.tensor):\n",
    "  max_len = max([len(s[0]) for s in samples]) if not max_len else max_len\n",
    "  padded_text = []\n",
    "  labels = []\n",
    "  for text, label in samples:\n",
    "    padded_text.append(\n",
    "        text + [PAD_IDX] * (max_len - len(text))\n",
    "       )\n",
    "    labels.append(label)\n",
    "  return (\n",
    "      to_tensor(padded_text) if to_tensor else padded_text,\n",
    "      to_tensor(labels)\n",
    "      )\n",
    "samples = [ ([1], 1), ([2, 3], 1), ([4, 5, 6], 0) ]\n",
    "pad(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1, 0],\n",
      "        [2, 3]]) tensor([1, 1])\n",
      "1 tensor([[ 4,  5,  6,  0],\n",
      "        [ 7,  8,  9, 10]]) tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "def make_batches(samples, batch_size):\n",
    "  batches = []\n",
    "  num_batches = len(samples) // batch_size\n",
    "  for i in range(num_batches):\n",
    "    batches.append(\n",
    "        pad(samples[i * batch_size : (i + 1) * batch_size])\n",
    "        )\n",
    "  return batches\n",
    "\n",
    "samples = [ ([1], 1), ([2, 3], 1), ([4, 5, 6], 0), ([7, 8, 9, 10], 1) ]\n",
    "batches = make_batches(samples, batch_size=2)\n",
    "for i, batch in enumerate(batches):\n",
    "    text, label = batch\n",
    "    print(i, text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Social Media Sentiment Dataset\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text  : torch.Size([3, 37]) tensor([[ 1,  1, 22,  1,  1,  1, 24,  1,  1, 22,  1,  1,  1,  1,  1,  1,  1,  1,\r\n",
      "          1,  1,  1,  1,  1, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "          0],\r\n",
      "        [ 1, 22,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 22,  1, 24,  1, 22,  1,\r\n",
      "          1,  1,  1, 24,  1, 22,  1,  1,  1,  5,  1,  1,  1,  1,  1,  5,  1,  1,\r\n",
      "         12],\r\n",
      "        [22,  1,  1,  1, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "          0]])\r\n",
      "Label : torch.Size([3]) tensor([0, 1, 1])\r\n"
     ]
    }
   ],
   "source": [
    "!python3 srmnlp/data/socialmedia.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercise\n",
    "---\n",
    "\n",
    "1. Use existing code to recreate sentences from preprocessed social media data\n",
    "2. Reduce the number of padding zeros by sorting the samples based on length, before batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous Representation\n",
    "---\n",
    "\n",
    "- word2vec\n",
    "    - (CBOW) Continous Bag-of-Words\n",
    "    - Skipgram\n",
    "- glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CBOW\n",
    "\n",
    "\n",
    "![](doc/cbow1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size, embed_dim):\n",
    "    super(CBOW, self).__init__()\n",
    "\n",
    "    # create embeddings with random initialization\n",
    "    self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "    # linear layer (1)\n",
    "    self.linear1 = nn.Linear(embed_dim, 128)\n",
    "    # linear layer (2)\n",
    "    self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    embedded = self.embeddings(inputs).sum(dim=1)\n",
    "    out = F.relu(self.linear1(embedded))\n",
    "    return F.log_softmax(self.linear2(out), dim=-1)\n",
    "\n",
    "  def get_embedding(self, idx):\n",
    "    if not isinstance(type(idx), type(torch.tensor(2.3))):\n",
    "      idx = torch.tensor(idx)\n",
    "    return self.embeddings(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data\n",
    "---\n",
    "\n",
    "```\n",
    "We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\n",
    "```\n",
    "---\n",
    "```python\n",
    "(['the', 'idea', 'a', 'computational'], 'of') \n",
    "(['to', 'study', 'idea', 'of'], 'the') \n",
    "(['processes', 'manipulate', 'abstract', 'things'], 'other')\n",
    "(['the', 'spirits', 'the', 'computer'], 'of')\n",
    "(['programs', 'to', 'processes.', 'In'], 'direct')\n",
    "(['a', 'pattern', 'rules', 'called'], 'of')\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing\n",
    "---\n",
    "\n",
    "```python\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [ words[i - 2], words[i - 1], words[i + 1], words[i + 2] ]\n",
    "    target = words[i]\n",
    "    samples.append((context, target))\n",
    "```\n",
    "---\n",
    "```python\n",
    "vocab = create_vocabulary(samples)[2:]  # we do not need PAD and UNK here\n",
    "w2i = vocab_to_w2i(vocab)\n",
    "label2idx = w2i\n",
    "samples = [ (x, label2idx[y]) for x, y in samples ]\n",
    "indexed_samples = [ (index(text, w2i), label) for text, label in samples ]\n",
    "return (\n",
    "  make_batches(indexed_samples, batch_size),\n",
    "  None\n",
    "  ), vocab, w2i\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training\n",
    "---\n",
    "\n",
    "```python\n",
    "def train(model, trainset, num_epochs=50):\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.\n",
    "        for context, target in trainset:\n",
    "            optim.zero_grad()\n",
    "            logloss = model(context)\n",
    "            loss = loss_fn(logloss, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch [{}], Loss : {}'.format(epoch, epoch_loss))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0], Loss : 238.19856929779053\n",
      "epoch [10], Loss : 1.45696222782135\n",
      "epoch [20], Loss : 0.37830962240695953\n",
      "epoch [30], Loss : 0.1671372950077057\n",
      "epoch [40], Loss : 0.0899118036031723\n",
      "Context Words :  ['People', 'create', 'to', 'direct']\n",
      "Target Word :  programs\n"
     ]
    }
   ],
   "source": [
    "!python3 srmnlp/cbow_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Skipgram\n",
    "\n",
    "![](doc/skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercise\n",
    "---\n",
    "\n",
    "- Implement Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment Analysis\n",
    "---\n",
    "\n",
    "- Model : LSTM based Recurrent Neural Network\n",
    "- Data  : Social Media Sentiment Corpus (from Kaggle)\n",
    "\n",
    "```bash\n",
    "# train-evaluate-predict\n",
    "python3 srmnlp/sentiment_analysis.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Epoch loss : 1.376315678541477, Epoch accuracy : 57.03125%\n",
      "::[evaluation] Loss : 0.6824692029219407, Accuracy : 56.875\n",
      "[2]\n",
      "Epoch loss : 1.3328340710737767, Epoch accuracy : 60.19631576538086%\n",
      "::[evaluation] Loss : 0.604145020705003, Accuracy : 67.45192307692308\n",
      "[3]\n",
      "Epoch loss : 0.9626376546728306, Epoch accuracy : 77.9647445678711%\n",
      "::[evaluation] Loss : 0.3938124108773011, Accuracy : 82.9326923076923\n",
      "[4]\n",
      "Epoch loss : 0.7804959511909729, Epoch accuracy : 83.47355651855469%\n",
      "::[evaluation] Loss : 0.38190830326997316, Accuracy : 85.0\n",
      "[5]\n",
      "Epoch loss : 0.7446358345257931, Epoch accuracy : 83.57371520996094%\n",
      "::[evaluation] Loss : 0.3639948526254067, Accuracy : 85.4326923076923\n",
      "[6]\n",
      "Epoch loss : 0.7185407674465424, Epoch accuracy : 84.19470977783203%\n",
      "::[evaluation] Loss : 0.3618682542672524, Accuracy : 85.24038461538461\n",
      "[7]\n",
      "Epoch loss : 0.6821818559979781, Epoch accuracy : 84.49519348144531%\n",
      "::[evaluation] Loss : 0.35941146543392766, Accuracy : 85.24038461538461\n",
      "[8]\n",
      "Epoch loss : 0.6524170946616393, Epoch accuracy : 84.85576629638672%\n",
      "::[evaluation] Loss : 0.35036762540157024, Accuracy : 85.04807692307692\n",
      "[9]\n",
      "Epoch loss : 0.6279343179403207, Epoch accuracy : 85.25640869140625%\n",
      "::[evaluation] Loss : 0.3574106814769598, Accuracy : 84.90384615384616\n",
      "[10]\n",
      "Epoch loss : 0.6129882093041371, Epoch accuracy : 85.45673370361328%\n",
      "::[evaluation] Loss : 0.3531854441532722, Accuracy : 84.90384615384616\n"
     ]
    }
   ],
   "source": [
    "!python3 srmnlp/sentiment_analysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "---\n",
    "\n",
    "- Try to improve the accuracy of Sentiment Classification on Social Media data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tasks\n",
    "---\n",
    "\n",
    "- <h4>Machine Translation</h4>\n",
    "- <h4>Question Answering</h4>\n",
    "- <h4>Text Summarization</h4>\n",
    "- <h4>Natural Language Inference</h4>\n",
    "- <h4>Goal Oriented Dialogue</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Translation\n",
    "\n",
    "![](doc/mt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Question Answering\n",
    "\n",
    "![](doc/qa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Summarization\n",
    "\n",
    "![](doc/summarization2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Natural Language Inference\n",
    "---\n",
    "\n",
    "- <h4>Entailment</h4>: If the first sentence is true, the second sentence must be true.\n",
    "- <h4>Contradiction</h4>: If the first sentence is true, the second sentence must be false.\n",
    "- <h4>Neutral</h4>: Neither entailment nor contradiction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Natural Language Inference\n",
    "\n",
    "![](doc/nli.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goal-Oriented Dialogue\n",
    "\n",
    "![](doc/god0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goal-Oriented Dialogue\n",
    "\n",
    "![](doc/god.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook notebook.ipynb to slides\n",
      "[NbConvertApp] Writing 413977 bytes to notebook.slides.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert notebook.ipynb --to slides --reveal-prefix ~/Desktop/tools/reveal.js"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
