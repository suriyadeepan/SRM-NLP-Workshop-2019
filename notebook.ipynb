{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "----\n",
    "\n",
    "<center><h3>Suriyadeepan Ramamoorthy</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "---\n",
    "\n",
    "- Text Represenation\n",
    "- Reduction / Abstraction\n",
    "- Articulation / Synthesis\n",
    "- Reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Represenation\n",
    "---\n",
    "- Count-based Representation\n",
    "- Continuous Represetation\n",
    "    - Example : Continuous Bag-of-Words\n",
    "- Text Preprocessing\n",
    "    - Example : Social Media Sentiment Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reduction / Abstraction\n",
    "---\n",
    "- Neural Networks\n",
    "- Example : Neural Networks from Scratch in pytorch\n",
    "- Recurrent Neural Networks\n",
    "    - LSTM (Long Short Term Memory)\n",
    "- Example : Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Count-based Representation\n",
    "---\n",
    "\n",
    "- One-hot Encoding\n",
    "- Bag-of-Words\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One-hot Encoding\n",
    "\n",
    "```python\n",
    "vocab = [ 'one', 'two', 'three' , 'four' ]\n",
    "```\n",
    "```\n",
    "one   : tensor([1., 0., 0., 0.])\n",
    "two   : tensor([0., 1., 0., 0.])\n",
    "three : tensor([0., 0., 1., 0.])\n",
    "four  : tensor([0., 0., 0., 1.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def one_hot_encode(w, w2i):\n",
    "  x = torch.zeros(len(w2i))\n",
    "  idx = w2i[w]\n",
    "  x[idx] = 1\n",
    "  return x\n",
    "\n",
    "vocab = [ 'one', 'two', 'three' , 'four' ]\n",
    "w2i = { w: i for i, w in enumerate(vocab) }\n",
    "for w in vocab:\n",
    "    print(w, ':', one_hot_encode(w, w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# N-gram\n",
    "\n",
    "```python\n",
    "sentence = 'one two three four'\n",
    "```\n",
    "\n",
    "```\n",
    "['one',\n",
    " 'two',\n",
    " 'three',\n",
    " 'four',\n",
    " 'one two',\n",
    " 'two three',\n",
    " 'three four',\n",
    " 'one two three',\n",
    " 'two three four']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('one two three four')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Count Vectorization\n",
    "\n",
    "\n",
    "```\n",
    "Size of vocabulary :  122\n",
    "Sentence :  The last question was asked for the first time, half in jest, on May 21, 2061, at a time when humanity first stepped into the light.\n",
    "Vector :  [[1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    "  0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0\n",
    "  0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 3 0 0 0\n",
    "  2 0 0 0 0 1 0 0 0 0 1 0 0 0]]\n",
    "Most Frequent Word :  (104, 'the')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# create an instance of sklearn's Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "sentences = sent_tokenize(open('data/asimov-tiny.txt').read())\n",
    "vectorizer.fit(sentences)\n",
    "w2i = vectorizer.vocabulary_\n",
    "vocab = { i: w for w, i in w2i.items() }\n",
    "print('Size of vocabulary : ', len(vocab))\n",
    "print('Sentence : ', sentences[0])\n",
    "sent_vector = vectorizer.transform([sentences[0]]).toarray()\n",
    "print('Vector : ', sent_vector)\n",
    "most_frequent = np.argmax(sent_vector[-1])\n",
    "print('Most Frequent Word : ', (w2i['the'], vocab[104]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TF-IDF\n",
    "---\n",
    "\n",
    "$$tf \\times idf$$\n",
    "- $tf$ : Term Frequency in the document\n",
    "- $idf$ : (logarithm of) inverse fraction of the documents that contain the word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# create an instance of sklearn's Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "sentences = sent_tokenize(open('data/asimov-tiny.txt').read())\n",
    "vectorizer.fit(sentences)\n",
    "vectors = vectorizer.transform(sentences)\n",
    "tf_transformer = TfidfTransformer()\n",
    "tf_transformer.fit(vectors)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "w2i = vectorizer.vocabulary_\n",
    "vocab = { i: w for w, i in w2i.items() }\n",
    "print('Size of vocabulary : ', len(vocab))\n",
    "print('Sentence : ', sentences[0])\n",
    "sent_vector = vectorizer.transform([sentences[0]]).toarray()\n",
    "print('Vector : ', sent_vector)\n",
    "most_frequent = np.argmax(sent_vector[-1])\n",
    "print(tf_transformer.transform(sent_vector).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "\n",
    "```\n",
    "[[0.19564323 0.19564323 0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.19564323 0.1639643  0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.39128647\n",
    "  0.         0.         0.1639643  0.         0.         0.\n",
    "  0.         0.         0.         0.         0.19564323 0.\n",
    "  0.         0.         0.         0.19564323 0.1639643  0.19564323\n",
    "  0.         0.         0.         0.19564323 0.         0.19564323\n",
    "  0.         0.         0.19564323 0.         0.         0.\n",
    "  0.         0.19564323 0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.19564323 0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.1639643  0.         0.         0.         0.\n",
    "  0.         0.         0.         0.         0.         0.19564323\n",
    "  0.         0.         0.26199672 0.         0.         0.\n",
    "  0.39128647 0.         0.         0.         0.         0.14148774\n",
    "  0.         0.         0.         0.         0.19564323 0.\n",
    "  0.         0.        ]]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch for Machine Learning\n",
    "---\n",
    "\n",
    "- Logistic Regression\n",
    "- Neural Network\n",
    "- Recurrent Neural Network\n",
    "- Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "---\n",
    "\n",
    "| x | y   |\n",
    "|------|------|\n",
    "| 0.54  | 2.01    |\n",
    "| 1.21  | 4.13    |\n",
    "| 0.2   | 0.82    |\n",
    "| ...   | ...     |\n",
    "\n",
    "$$\\hat{y} = wx + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.rand(1)\n",
    "linear = nn.Linear(1, 1)\n",
    "y = linear(x)\n",
    "print('x : ', x)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ', list(linear.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "```\n",
    "x :  tensor([0.3726])\n",
    "y :  tensor([-0.6245], grad_fn=<AddBackward0>)\n",
    "Associated Parameters :  [('weight', Parameter containing:\n",
    "tensor([[0.1961]], requires_grad=True)), ('bias', Parameter containing:\n",
    "tensor([-0.6975], requires_grad=True))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-variate Linear Regression\n",
    "---\n",
    "\n",
    "| $x_0$ | $x_1$   | $x_2$ | y   |\n",
    "|------|------|------|------|\n",
    "| 0.54  | 2.01    | 0.14  | 2.91    |\n",
    "| 1.21  | 4.13    | 0.24  | 4.22    |\n",
    "| 0.2   | 0.82    | 1.8   | 1.35    |\n",
    "| ...   | ...     | ...   | ...     |\n",
    "\n",
    "$$\\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b$$\n",
    "<center><h3>OR</h3></center>\n",
    "$$\\hat{\\overrightarrow{y}} = W\\overrightarrow{x} + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.rand(1, 3)\n",
    "linear = nn.Linear(3, 1)\n",
    "y = linear(x)\n",
    "print('x : ', x)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ', list(linear.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-variate Linear Regression\n",
    "\n",
    "```\n",
    "x :  tensor([[0.2249, 0.2030, 0.8778]])\n",
    "y :  tensor([[0.0854]], grad_fn=<AddmmBackward>)\n",
    "Associated Parameters :  [('weight', Parameter containing:\n",
    "tensor([[ 0.4610, -0.3530,  0.5720]], requires_grad=True)), ('bias', Parameter containing:\n",
    "tensor([-0.4488], requires_grad=True))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "---\n",
    "$$\\hat{y} = \\sigma( wx + b )$$\n",
    "$$ y \\epsilon (0, 1)\\ \\forall\\ x \\epsilon (-\\infty, +\\infty) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "x = torch.rand(1)\n",
    "linear = nn.Linear(1, 1)\n",
    "activation_fn = nn.Sigmoid()\n",
    "# activation_fn = nn.Tanh()\n",
    "y = activation_fn(linear(x))\n",
    "print('x : ', x)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ', list(linear.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "```\n",
    "x :  tensor([0.4730])\n",
    "y :  tensor([0.6162], grad_fn=<SigmoidBackward>)\n",
    "Associated Parameters :  [('weight', Parameter containing:\n",
    "tensor([[0.1867]], requires_grad=True)), ('bias', Parameter containing:\n",
    "tensor([0.3851], requires_grad=True))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Network\n",
    "---\n",
    "\n",
    "- Feed-Forward Neural Network\n",
    "- Add a hidden layer\n",
    "- 1 input layer, 1 hidden layer, 1 output layer\n",
    "\n",
    "$$ \\hat{y} = \\tanh(W_2z + b_2) $$\n",
    "$$ z = \\tanh(W_2x + b_1) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "x = torch.rand(1, 3)\n",
    "linear_1 = nn.Linear(3, 5)\n",
    "activation_fn_1 = nn.Tanh()\n",
    "linear_2 = nn.Linear(5, 1)\n",
    "activation_fn_2 = nn.Tanh()\n",
    "z = activation_fn_1(linear_1(x))\n",
    "y = activation_fn_2(linear_2(z))\n",
    "print('x : ', x)\n",
    "print('z : ', z)\n",
    "print('y : ', y)\n",
    "print('Associated Parameters : ')\n",
    "print('\\nlinear_1 (weight) : ', \n",
    "      linear_1.weight.size(), linear_1.weight)\n",
    "print('linear_1 (bias) : ', \n",
    "      linear_1.bias.size(), linear_1.bias)\n",
    "print('\\nlinear_2 (weight) : ', \n",
    "      linear_2.weight.size(), linear_2.weight)\n",
    "print('linear_2 (bias) : ', \n",
    "      linear_2.bias.size(), linear_2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "```\n",
    "linear_1 (weight) :  torch.Size([5, 3]) Parameter containing:\n",
    "tensor([[ 0.0067, -0.0129, -0.3105],\n",
    "        [ 0.0487, -0.5520, -0.5208],\n",
    "        [-0.5579, -0.0674, -0.3417],\n",
    "        [-0.2760, -0.2374,  0.4286],\n",
    "        [ 0.1238,  0.5067, -0.0352]], requires_grad=True)\n",
    "linear_1 (bias) :  torch.Size([5]) Parameter containing:\n",
    "tensor([-0.4454,  0.5320,  0.0727,  0.1670, -0.0999], requires_grad=True)\n",
    "\n",
    "linear_2 (weight) :  torch.Size([1, 5]) Parameter containing:\n",
    "tensor([[ 0.1594,  0.2691, -0.1408,  0.1989,  0.4021]], requires_grad=True)\n",
    "linear_2 (bias) :  torch.Size([1]) Parameter containing:\n",
    "tensor([0.2987], requires_grad=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook notebook.ipynb to slides\n",
      "[NbConvertApp] Writing 360427 bytes to notebook.slides.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert notebook.ipynb --to slides --reveal-prefix ~/Desktop/tools/reveal.js"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
